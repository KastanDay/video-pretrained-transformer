{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kastan/utils/miniconda3/envs/trying_cockpit/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import jsonlines\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import numpy as np\n",
    "import pathlib\n",
    "from numpy import load\n",
    "import torch\n",
    "import lovely_tensors as lt\n",
    "lt.monkey_patch()\n",
    "import tqdm\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate clip\n",
    "import clip\n",
    "\n",
    "MODEL_SIZE = 'ViT-L/14@336px'  # Best models are (1st) ViT-L/14@336px and (2nd) ViT-L/14. I don't recommend going lower.  \n",
    "clip_instance, clip_preprocess = clip.load(MODEL_SIZE, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T5\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Model, T5Config, AutoModelWithLMHead\n",
    "\n",
    "'''\n",
    "MODEL SELECTION\n",
    "\n",
    "T5 V1.1 --  https://huggingface.co/docs/transformers/model_doc/t5v1.1 && https://github.com/google-research/text-to-text-transfer-transformer/blob/main/released_checkpoints.md#t511\n",
    "small - base - large - 3b/xl - 11b/xxl\n",
    "\n",
    "OG: t5-small\n",
    "\n",
    "'google/t5-base-lm-adapt' # largest on my server (without float16)\n",
    "'google/t5-xl-lm-adapt'\n",
    "\n",
    "google/t5-v1_1-large\n",
    "'''\n",
    "\n",
    "# MODEL_SIZE = \"t5-base\"\n",
    "MODEL_NAME = \"google/t5-v1_1-base\"\n",
    "# MODEL_NAME = \"google/t5-base-lm-adapt\"\n",
    "# config = T5Config.from_pretrained(MODEL_NAME)\n",
    "t5 = T5ForConditionalGeneration.from_pretrained(MODEL_NAME, torch_dtype=torch.float32, low_cpu_mem_usage=False).to(device) # float16, True\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME, return_special_tokens_mask=True)\n",
    "# low_cpu_mem_usage(bool, optional) â€” Tries to not use more than 1x model size in CPU memory (including peak memory) while loading the model. experimental.\n",
    "optimizer = torch.optim.Adam(params =  t5.parameters(), lr=1e-4) # Typically, 1e-4 and 3e-4 work well for most problems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cockpit ML Debugger\n",
    "# https://cockpit.readthedocs.io/en/latest/examples/01_basic_fmnist.html\n",
    "from backpack import extend\n",
    "from cockpit import Cockpit, CockpitPlotter\n",
    "from cockpit.utils.configuration import configuration\n",
    "\n",
    "t5 = extend(t5)\n",
    "\n",
    "individual_loss_fn = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "# Create Cockpit and a plotter\n",
    "cockpit = Cockpit(t5.parameters(), quantities=configuration(\"full\"))\n",
    "plotter = CockpitPlotter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name = \"parallel_15\"\n",
    "REMOTE_WHISPER_FILE = f'/mnt/storage_hdd/thesis/yt_1b_dataset/yt_1b_train/{dir_name}_whisper_output.jsonl'\n",
    "REMOTE_CLIP_DIR  = f'/mnt/storage_hdd/thesis/yt_1b_dataset/yt_1b_train/{dir_name}_clip_output'\n",
    "REMOTE_SCENE_FILE = f'/mnt/storage_hdd/thesis/yt_1b_dataset/yt_1b_train/{dir_name}_scene_output.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shapes:\n",
      "tensor[768] f16 xâˆˆ[-13.586, 11.469] Î¼=-0.000 Ïƒ=0.743 cuda:0\n",
      "tensor[768] f16 xâˆˆ[-6.023, 6.324] Î¼=-0.015 Ïƒ=0.631 cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss\n",
      "tensor grad NllLossBackward0 cuda:0 17.655\n",
      "logits\n",
      "tensor[1, 33, 32128] n=1060224 xâˆˆ[-96.418, 3.408] Î¼=-27.929 Ïƒ=9.032 grad BackwardHookFunctionBackward cuda:0\n",
      "individual_losses\n",
      "tensor grad NllLossBackward0 cuda:0 17.655\n",
      "loss\n",
      "logits\n",
      "past_key_values\n",
      "encoder_last_hidden_state\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 85\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[39mwith\u001b[39;00m cockpit(\n\u001b[1;32m     77\u001b[0m     global_step,\n\u001b[1;32m     78\u001b[0m     info\u001b[39m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     83\u001b[0m     },\n\u001b[1;32m     84\u001b[0m ):\n\u001b[0;32m---> 85\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward(create_graph\u001b[39m=\u001b[39;49mcockpit\u001b[39m.\u001b[39;49mcreate_graph(global_step))\n\u001b[1;32m     87\u001b[0m \u001b[39m# optimizer step\u001b[39;00m\n",
      "File \u001b[0;32m~/utils/miniconda3/envs/trying_cockpit/lib/python3.8/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/utils/miniconda3/envs/trying_cockpit/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/utils/miniconda3/envs/trying_cockpit/lib/python3.8/site-packages/torch/utils/hooks.py:110\u001b[0m, in \u001b[0;36mBackwardHook._set_user_hook.<locals>.hook\u001b[0;34m(grad_input, _)\u001b[0m\n\u001b[1;32m    109\u001b[0m grad_input \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pack_with_none(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_tensors_index, grad_input, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_inputs)\n\u001b[0;32m--> 110\u001b[0m res \u001b[39m=\u001b[39m user_hook(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodule, grad_input, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgrad_outputs)\n\u001b[1;32m    111\u001b[0m \u001b[39mif\u001b[39;00m res \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/utils/miniconda3/envs/trying_cockpit/lib/python3.8/site-packages/backpack/__init__.py:209\u001b[0m, in \u001b[0;36mhook_run_extensions\u001b[0;34m(module, g_inp, g_out)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m[DEBUG] Running extension\u001b[39m\u001b[39m\"\u001b[39m, backpack_extension, \u001b[39m\"\u001b[39m\u001b[39mon\u001b[39m\u001b[39m\"\u001b[39m, module)\n\u001b[0;32m--> 209\u001b[0m     backpack_extension(module, g_inp, g_out)\n\u001b[1;32m    211\u001b[0m \u001b[39mif\u001b[39;00m debug:\n",
      "File \u001b[0;32m~/utils/miniconda3/envs/trying_cockpit/lib/python3.8/site-packages/backpack/extensions/backprop_extension.py:127\u001b[0m, in \u001b[0;36mBackpropExtension.__call__\u001b[0;34m(self, module, g_inp, g_out)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[39mif\u001b[39;00m module_extension \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 127\u001b[0m     module_extension(\u001b[39mself\u001b[39;49m, module, g_inp, g_out)\n",
      "File \u001b[0;32m~/utils/miniconda3/envs/trying_cockpit/lib/python3.8/site-packages/backpack/extensions/module_extension.py:106\u001b[0m, in \u001b[0;36mModuleExtension.__call__\u001b[0;34m(self, extension, module, g_inp, g_out)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    102\u001b[0m     extension\u001b[39m.\u001b[39mexpects_backpropagation_quantities()\n\u001b[1;32m    103\u001b[0m     \u001b[39mand\u001b[39;00m bp_quantity \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    104\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_loss(module)\n\u001b[1;32m    105\u001b[0m ):\n\u001b[0;32m--> 106\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[1;32m    107\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mBackPACK extension expects a backpropagation quantity but it is None. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    108\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModule: \u001b[39m\u001b[39m{\u001b[39;00mmodule\u001b[39m}\u001b[39;00m\u001b[39m, Extension: \u001b[39m\u001b[39m{\u001b[39;00mextension\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    109\u001b[0m     )\n\u001b[1;32m    111\u001b[0m \u001b[39mfor\u001b[39;00m param \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__params:\n",
      "\u001b[0;31mAssertionError\u001b[0m: BackPACK extension expects a backpropagation quantity but it is None. Module: Linear(in_features=768, out_features=32128, bias=False), Extension: <backpack.extensions.secondorder.diag_hessian.DiagHessian object at 0x7f629f60a7f0>.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 85\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[39m# loss.sum().backward()\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[39m# backward pass\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[39mwith\u001b[39;00m cockpit(\n\u001b[1;32m     77\u001b[0m     global_step,\n\u001b[1;32m     78\u001b[0m     info\u001b[39m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     83\u001b[0m     },\n\u001b[1;32m     84\u001b[0m ):\n\u001b[0;32m---> 85\u001b[0m     loss\u001b[39m.\u001b[39mbackward(create_graph\u001b[39m=\u001b[39mcockpit\u001b[39m.\u001b[39mcreate_graph(global_step))\n\u001b[1;32m     87\u001b[0m \u001b[39m# optimizer step\u001b[39;00m\n\u001b[1;32m     88\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/githubs/cockpit/cockpit/context.py:155\u001b[0m, in \u001b[0;36mBackwardCTX.__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[39mfor\u001b[39;00m ctx \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontexts:\n\u001b[1;32m    153\u001b[0m     ctx\u001b[39m.\u001b[39m\u001b[39m__exit__\u001b[39m(\u001b[39mtype\u001b[39m, value, traceback)\n\u001b[0;32m--> 155\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcp\u001b[39m.\u001b[39;49mtrack(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mglobal_step, protected_savefields\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprotected_savefields)\n\u001b[1;32m    157\u001b[0m CockpitCTX\u001b[39m.\u001b[39merase()\n",
      "File \u001b[0;32m~/githubs/cockpit/cockpit/cockpit.py:195\u001b[0m, in \u001b[0;36mCockpit.track\u001b[0;34m(self, global_step, protected_savefields)\u001b[0m\n\u001b[1;32m    190\u001b[0m before_cleanup \u001b[39m=\u001b[39m [\n\u001b[1;32m    191\u001b[0m     q \u001b[39mfor\u001b[39;00m q \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquantities \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(q, quantities\u001b[39m.\u001b[39mHessMaxEV)\n\u001b[1;32m    192\u001b[0m ]\n\u001b[1;32m    194\u001b[0m \u001b[39mfor\u001b[39;00m q \u001b[39min\u001b[39;00m before_cleanup:\n\u001b[0;32m--> 195\u001b[0m     q\u001b[39m.\u001b[39;49mtrack(global_step, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparams, batch_loss)\n\u001b[1;32m    197\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_free_backpack_buffers(global_step, protected_savefields)\n\u001b[1;32m    199\u001b[0m after_cleanup \u001b[39m=\u001b[39m [\n\u001b[1;32m    200\u001b[0m     q \u001b[39mfor\u001b[39;00m q \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquantities \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(q, quantities\u001b[39m.\u001b[39mHessMaxEV)\n\u001b[1;32m    201\u001b[0m ]\n",
      "File \u001b[0;32m~/githubs/cockpit/cockpit/quantities/quantity.py:101\u001b[0m, in \u001b[0;36mQuantity.track\u001b[0;34m(self, global_step, params, batch_loss)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[39m\"\"\"Perform scheduled computations and store result.\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \n\u001b[1;32m     94\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[39m    batch_loss (torch.Tensor): Mini-batch loss from current step.\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshould_compute(global_step):\n\u001b[0;32m--> 101\u001b[0m     iteration, result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute(global_step, params, batch_loss)\n\u001b[1;32m    102\u001b[0m     \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    103\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_verbose:\n",
      "File \u001b[0;32m~/githubs/cockpit/cockpit/quantities/quantity.py:530\u001b[0m, in \u001b[0;36mTwoStepQuantity.compute\u001b[0;34m(self, global_step, params, batch_loss)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[39m\"\"\"Evaluate quantity at a step in training.\u001b[39;00m\n\u001b[1;32m    514\u001b[0m \n\u001b[1;32m    515\u001b[0m \u001b[39mAfter the computation, temporarily cached info is deleted from the internal\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    527\u001b[0m \u001b[39m    quantities whose values are computed in later iterations).\u001b[39;00m\n\u001b[1;32m    528\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    529\u001b[0m save_iter \u001b[39m=\u001b[39m global_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mSAVE_SHIFT\n\u001b[0;32m--> 530\u001b[0m save_result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_compute(global_step, params, batch_loss)\n\u001b[1;32m    532\u001b[0m \u001b[39m# assume next iteration already started to discard irrelevant information\u001b[39;00m\n\u001b[1;32m    533\u001b[0m virtual_step \u001b[39m=\u001b[39m global_step \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/githubs/cockpit/cockpit/quantities/quantity.py:552\u001b[0m, in \u001b[0;36mTwoStepQuantity._compute\u001b[0;34m(self, global_step, params, batch_loss)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[39m\"\"\"Perform start and end point computation if necessary.\u001b[39;00m\n\u001b[1;32m    540\u001b[0m \n\u001b[1;32m    541\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[39m    intermediate information rather than computing the quantity's value.\u001b[39;00m\n\u001b[1;32m    550\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    551\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_start(global_step):\n\u001b[0;32m--> 552\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_compute_start(global_step, params, batch_loss)\n\u001b[1;32m    554\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_end(global_step):\n",
      "File \u001b[0;32m~/githubs/cockpit/cockpit/quantities/alpha.py:289\u001b[0m, in \u001b[0;36mAlpha._compute_start\u001b[0;34m(self, global_step, params, batch_loss)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[39m# fall back to storing parameters and individual gradients at start/end\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__projection_with_backpack(global_step):\n\u001b[0;32m--> 289\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_save_1st_order_info(global_step, params, batch_loss, point, until)\n",
      "File \u001b[0;32m~/githubs/cockpit/cockpit/quantities/alpha.py:335\u001b[0m, in \u001b[0;36mAlpha._save_1st_order_info\u001b[0;34m(self, global_step, params, batch_loss, point, until)\u001b[0m\n\u001b[1;32m    332\u001b[0m params_dict \u001b[39m=\u001b[39m {\u001b[39mid\u001b[39m(p): p\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mclone()\u001b[39m.\u001b[39mdetach() \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m params}\n\u001b[1;32m    333\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_to_cache(global_step, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparams_\u001b[39m\u001b[39m{\u001b[39;00mpoint\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, params_dict, block_fn)\n\u001b[0;32m--> 335\u001b[0m grad_dict \u001b[39m=\u001b[39m {\u001b[39mid\u001b[39m(p): p\u001b[39m.\u001b[39mgrad\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mclone()\u001b[39m.\u001b[39mdetach() \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m params}\n\u001b[1;32m    336\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_to_cache(global_step, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgrad_\u001b[39m\u001b[39m{\u001b[39;00mpoint\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, grad_dict, block_fn)\n\u001b[1;32m    338\u001b[0m \u001b[39m# L = Â¹/â‚™ âˆ‘áµ¢ â„“áµ¢, BackPACK's BatchGrad computes Â¹/â‚™ âˆ‡â„“áµ¢, we have to rescale\u001b[39;00m\n",
      "File \u001b[0;32m~/githubs/cockpit/cockpit/quantities/alpha.py:335\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    332\u001b[0m params_dict \u001b[39m=\u001b[39m {\u001b[39mid\u001b[39m(p): p\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mclone()\u001b[39m.\u001b[39mdetach() \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m params}\n\u001b[1;32m    333\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_to_cache(global_step, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparams_\u001b[39m\u001b[39m{\u001b[39;00mpoint\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, params_dict, block_fn)\n\u001b[0;32m--> 335\u001b[0m grad_dict \u001b[39m=\u001b[39m {\u001b[39mid\u001b[39m(p): p\u001b[39m.\u001b[39;49mgrad\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39mclone()\u001b[39m.\u001b[39mdetach() \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m params}\n\u001b[1;32m    336\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_to_cache(global_step, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgrad_\u001b[39m\u001b[39m{\u001b[39;00mpoint\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, grad_dict, block_fn)\n\u001b[1;32m    338\u001b[0m \u001b[39m# L = Â¹/â‚™ âˆ‘áµ¢ â„“áµ¢, BackPACK's BatchGrad computes Â¹/â‚™ âˆ‡â„“áµ¢, we have to rescale\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'data'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "''' just for trying the debugger cockpit'''\n",
    "\n",
    "\n",
    "# Iterate through the batch\n",
    "clip_15 = os.listdir(REMOTE_CLIP_DIR)\n",
    "\n",
    "# Initialize embeddings\n",
    "one_input_shape = [1, 768, 768]\n",
    "att_mask_shape = [1, 768]\n",
    "embed_shape = [1, 768]\n",
    "\n",
    "input_embeds_arr = torch.zeros(one_input_shape).to(device) # .astype(np.float16)\n",
    "attn_mask_arr    = torch.zeros(att_mask_shape).to(device)\n",
    "attn_mask_arr[0][0] = 1\n",
    "attn_mask_arr[0][1] = 1\n",
    "attn_mask_arr[0][2] = 1 # no clip\n",
    "\n",
    "t5.train()\n",
    "\n",
    "global_step = 0 \n",
    "\n",
    "with jsonlines.open(REMOTE_SCENE_FILE, 'r') as scene_reader:\n",
    "    # Zipping the scene graph with the clip + whisper embeddings\n",
    "    \n",
    "    # itr over videos\n",
    "    for scene_seg_list, clip_npz_path in tqdm.tqdm(zip(scene_reader, glob.glob(os.path.join(REMOTE_CLIP_DIR, '*'), recursive = True))):\n",
    "        try:\n",
    "            np_loaded = np.load(clip_npz_path, allow_pickle=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load compressed numpy: {e}\")\n",
    "            continue\n",
    "        object_list_of_str = []\n",
    "        scene_seg_list = json.loads(scene_seg_list)\n",
    "        \n",
    "        # iterate over segments\n",
    "        for segment_index in range(np_loaded['arr_0'].item()['total_segments']):\n",
    "            # print(np_loaded[f'arr_{segment_index}'].item()['captions'])\n",
    "            frame_embedding       = np_loaded[f'arr_{segment_index}'].item()['frame_embeddings']\n",
    "            caption_embedding     = np_loaded[f'arr_{segment_index}'].item()['text_caption_embeddings']\n",
    "            whisper_text_captions = np_loaded[f'arr_{segment_index}'].item()['captions']\n",
    "            \n",
    "            frame_embedding       = torch.from_numpy(frame_embedding.reshape((768,))).to(device)\n",
    "            caption_embedding     = torch.from_numpy(caption_embedding).to(device)\n",
    "\n",
    "            # Update embedding array\n",
    "            input_embeds_arr[0][0] = frame_embedding\n",
    "            input_embeds_arr[0][1] = caption_embedding\n",
    "            # Set to torch\n",
    "            decoder_input_embeds_arr = np.random.rand( *one_input_shape )  # .astype(np.float16) # need fp32\n",
    "            decoder_input_embeds_arr = decoder_input_embeds_arr\n",
    "            input_embeds_arr = input_embeds_arr\n",
    "            attn_mask_arr = attn_mask_arr\n",
    "            \n",
    "            print(\"Input shapes:\")\n",
    "            print(caption_embedding)\n",
    "            print(frame_embedding)\n",
    "            labels = t5_tokenizer(whisper_text_captions, return_tensors=\"pt\").input_ids.to(device)\n",
    "            # outputs = t5.forward(inputs_embeds=input_embeds_arr, attention_mask=attn_mask_arr, decoder_inputs_embeds=input_embeds_arr)\n",
    "            outputs = t5.forward(inputs_embeds=input_embeds_arr, attention_mask=attn_mask_arr, labels=labels, return_dict=True)\n",
    "            # outputs = t5.forward(inputs_embeds=input_embeds_arr, labels=labels)\n",
    "            loss = outputs[0]\n",
    "            print(\"loss\")\n",
    "            print(loss)\n",
    "            logits = outputs[1]\n",
    "            print(\"logits\")\n",
    "            print(logits)\n",
    "            individual_losses = individual_loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "            print(\"individual_losses\")\n",
    "            print(individual_losses)\n",
    "            # loss = \n",
    "            for item in outputs:\n",
    "                print(item)\n",
    "            ''' backwards pass '''\n",
    "            # loss.sum().backward()\n",
    "            # backward pass\n",
    "            with cockpit(\n",
    "                global_step,\n",
    "                info={\n",
    "                    \"batch_size\": one_input_shape[0],\n",
    "                    \"individual_losses\": individual_losses,\n",
    "                    \"loss\": loss,\n",
    "                    \"optimizer\": optimizer,\n",
    "                },\n",
    "            ):\n",
    "                loss.backward(create_graph=cockpit.create_graph(global_step))\n",
    "            \n",
    "            # optimizer step\n",
    "            optimizer.zero_grad()\n",
    "            optimizer.step()\n",
    "            global_step += 1\n",
    "            print(f\"step: {global_step}\")\n",
    "            plotter.plot(cockpit)\n",
    "            \n",
    "            print(\"Loss ðŸ‘‡ðŸ‘‡ðŸ‘‡\")\n",
    "            print(loss)\n",
    "            break\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "''' More complete '''\n",
    "\n",
    "# Iterate through the batch\n",
    "clip_15 = os.listdir(REMOTE_CLIP_DIR)\n",
    "\n",
    "# Initialize embeddings\n",
    "one_input_shape = [1, 768, 768]\n",
    "att_mask_shape = [1, 768]\n",
    "embed_shape = [1, 768]\n",
    "\n",
    "input_embeds_arr = torch.zeros(one_input_shape).to(device) # .astype(np.float16)\n",
    "attn_mask_arr    = torch.zeros(att_mask_shape).to(device)\n",
    "attn_mask_arr[0][0] = 1\n",
    "attn_mask_arr[0][1] = 1\n",
    "attn_mask_arr[0][2] = 1 # no clip\n",
    "\n",
    "t5.train()\n",
    "\n",
    "global_step = 0 \n",
    "\n",
    "with jsonlines.open(REMOTE_SCENE_FILE, 'r') as scene_reader:\n",
    "    # Zipping the scene graph with the clip + whisper embeddings\n",
    "    \n",
    "    # itr over videos\n",
    "    for scene_seg_list, clip_npz_path in tqdm.tqdm(zip(scene_reader, glob.glob(os.path.join(REMOTE_CLIP_DIR, '*'), recursive = True))):\n",
    "        try:\n",
    "            np_loaded = np.load(clip_npz_path, allow_pickle=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load compressed numpy: {e}\")\n",
    "            continue\n",
    "        object_list_of_str = []\n",
    "        scene_seg_list = json.loads(scene_seg_list)\n",
    "        \n",
    "        # iterate over segments\n",
    "        for segment_index in range(np_loaded['arr_0'].item()['total_segments']):\n",
    "            frame_embedding       = np_loaded[f'arr_{segment_index}'].item()['frame_embeddings']\n",
    "            caption_embedding     = np_loaded[f'arr_{segment_index}'].item()['text_caption_embeddings']\n",
    "            whisper_text_captions = np_loaded[f'arr_{segment_index}'].item()['captions']\n",
    "            \n",
    "            frame_embedding       = torch.from_numpy(frame_embedding.reshape((768,))).to(device)\n",
    "            caption_embedding     = torch.from_numpy(caption_embedding).to(device)\n",
    "\n",
    "            scene_caption = scene_seg_list[segment_index]\n",
    "            scene_caption = clip.tokenize(scene_caption).to(device)\n",
    "            with torch.inference_mode(): # even faster than no_grad()\n",
    "                scene_embedding = clip_instance.encode_text(scene_caption)\n",
    "            scene_embedding = scene_embedding.reshape((768,))\n",
    "\n",
    "            # Update embedding array\n",
    "            input_embeds_arr[0][0] = frame_embedding\n",
    "            input_embeds_arr[0][1] = caption_embedding\n",
    "            input_embeds_arr[0][2] = scene_embedding\n",
    "            # Set to torch\n",
    "            decoder_input_embeds_arr = np.random.rand( *one_input_shape )  # .astype(np.float16) # need fp32\n",
    "            decoder_input_embeds_arr = decoder_input_embeds_arr\n",
    "            input_embeds_arr = input_embeds_arr\n",
    "            attn_mask_arr = attn_mask_arr\n",
    "            \n",
    "            print(\"Input shapes:\")\n",
    "            print(scene_embedding)\n",
    "            print(caption_embedding)\n",
    "            print(frame_embedding)\n",
    "            labels = t5_tokenizer(whisper_text_captions, return_tensors=\"pt\").input_ids.to(device)\n",
    "            # outputs = t5.forward(inputs_embeds=input_embeds_arr, attention_mask=attn_mask_arr, decoder_inputs_embeds=input_embeds_arr)\n",
    "            outputs = t5.forward(inputs_embeds=input_embeds_arr, attention_mask=attn_mask_arr, labels=labels, return_dict=True)\n",
    "            # outputs = t5.forward(inputs_embeds=input_embeds_arr, labels=labels)\n",
    "            loss = outputs[0]\n",
    "            individual_losses = outputs[-1]\n",
    "            for item in outputs:\n",
    "                print(item)\n",
    "            ''' backwards pass '''\n",
    "            # loss.sum().backward()\n",
    "            # backward pass\n",
    "            with cockpit(\n",
    "                global_step,\n",
    "                info={\n",
    "                    \"batch_size\": one_input_shape[0],\n",
    "                    \"individual_losses\": individual_losses,\n",
    "                    \"loss\": loss,\n",
    "                    \"optimizer\": optimizer,\n",
    "                },\n",
    "            ):\n",
    "                loss.sum().backward(create_graph=cockpit.create_graph(global_step))\n",
    "            \n",
    "            # optimizer step\n",
    "            optimizer.zero_grad()\n",
    "            optimizer.step()\n",
    "            global_step += 1\n",
    "            print(f\"step: {global_step}\")\n",
    "            plotter.plot(cockpit)\n",
    "            \n",
    "            print(\"Loss ðŸ‘‡ðŸ‘‡ðŸ‘‡\")\n",
    "            print(loss)\n",
    "            break\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t5.save_pretrained(\"BIG_PENIS_PREVAILS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying Inference with custom model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import jsonlines\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import numpy as np\n",
    "import pathlib\n",
    "from numpy import load\n",
    "import lovely_tensors as lt\n",
    "lt.monkey_patch()\n",
    "import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import jsonlines\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import numpy as np\n",
    "import pathlib\n",
    "from numpy import load\n",
    "import lovely_tensors as lt\n",
    "lt.monkey_patch()\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import numpy as np\n",
    "import pathlib\n",
    "from numpy import load\n",
    "import torch\n",
    "import lovely_tensors as lt\n",
    "lt.monkey_patch()\n",
    "import tqdm\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T5\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Model, T5Config, AutoModelWithLMHead\n",
    "\n",
    "'''\n",
    "MODEL SELECTION\n",
    "\n",
    "T5 V1.1 --  https://huggingface.co/docs/transformers/model_doc/t5v1.1 && https://github.com/google-research/text-to-text-transfer-transformer/blob/main/released_checkpoints.md#t511\n",
    "small - base - large - 3b/xl - 11b/xxl\n",
    "\n",
    "OG: t5-small\n",
    "\n",
    "'google/t5-base-lm-adapt' # largest on my server (without float16)\n",
    "'google/t5-xl-lm-adapt'\n",
    "\n",
    "google/t5-v1_1-large\n",
    "'''\n",
    "\n",
    "# MODEL_SIZE = \"t5-base\"\n",
    "MODEL_NAME = \"google/t5-v1_1-base\"\n",
    "OUR_FINETUNED_MODEL = \"\"\n",
    "# MODEL_NAME = \"google/t5-base-lm-adapt\"\n",
    "# config = T5Config.from_pretrained(MODEL_NAME)\n",
    "t5 = T5ForConditionalGeneration.from_pretrained(MODEL_NAME, torch_dtype=torch.float32, low_cpu_mem_usage=False).to(device) # float16, True\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME, return_special_tokens_mask=True)\n",
    "# low_cpu_mem_usage(bool, optional) â€” Tries to not use more than 1x model size in CPU memory (including peak memory) while loading the model. experimental.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "# model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "# training\n",
    "# input_ids = t5_tokenizer(\"The <extra_id_0> walks in <extra_id_1> park\", return_tensors=\"pt\").input_ids\n",
    "# labels = t5_tokenizer(\"<extra_id_0> cute dog <extra_id_1> the <extra_id_2>\", return_tensors=\"pt\").input_ids\n",
    "# outputs = t5(input_ids=input_ids, labels=labels)\n",
    "# loss = outputs.loss\n",
    "# logits = outputs.logits\n",
    "\n",
    "# inference\n",
    "input_ids = t5_tokenizer(\n",
    "    \"Hey guys welcome back to the channel\", return_tensors=\"pt\"\n",
    ").input_ids  # Batch size 1\n",
    "outputs = t5.generate(input_ids)\n",
    "print(\"Here's the output ðŸ‘‡ðŸ‘‡ðŸ‘‡\")\n",
    "print(t5_tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "# studies have shown that owning a dog is good for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trying_cockpit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15 (default, Nov 24 2022, 15:19:38) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ce55a297c26f6fe04c66c84d702c764c0bcdcad004aba573147a8b3840800a75"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
